{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b1edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import json, random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# Ensures predictable results\n",
    "seed = 42\n",
    "random.seed(42)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Configure Torch device\n",
    "device = \"cpu\"\n",
    "#device = \"cuda\" if torch.cuda_is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aa064a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DogsDataset class\n",
    "class DogsDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, transform=None, class_to_idx=None):\n",
    "        # Define path to data\n",
    "        self.path = Path(jsonl_path)\n",
    "        # Quite literally transforming the image\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load JSONL\n",
    "        with self.path.open() as f:\n",
    "            # Confusing JSONL loading\n",
    "            self.items = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "        # Create or assign Class Mapping (e.g. \"0\" = Chihuahua)\n",
    "        if class_to_idx is None:\n",
    "            # Ensures deterministic class order\n",
    "            classes = sorted({x[\"class_name\"] for x in self.items})\n",
    "            # Assigns a number to each class\n",
    "            self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "    \n",
    "    # Returns No. of data points\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "\n",
    "        # Convert to RGB image\n",
    "        img = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "        label = self.class_to_idx[item[\"class_name\"]]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff9dbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "image_size = 224\n",
    "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "# Make training data harder for better learning\n",
    "train_tf = T.Compose([\n",
    "    T.RandomResizedCrop(image_size),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])\n",
    "\n",
    "# Validation/Test data gets standardized\n",
    "eval_tf = T.Compose([\n",
    "    T.Resize(int(image_size * 1.14)),\n",
    "    T.CenterCrop(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "707c4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Datasets and DataLoaders \n",
    "train_ds = DogsDataset(\"data/splits/dogs_train.jsonl\", transform=train_tf)\n",
    "# To ensure consistency the same class_to_idx has been used in all datasets\n",
    "val_ds = DogsDataset(\"data/splits/dogs_val.jsonl\", transform=eval_tf, class_to_idx=train_ds.class_to_idx)\n",
    "test_ds = DogsDataset(\"data/splits/dogs_test.jsonl\", transform=eval_tf, class_to_idx=train_ds.class_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e14eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets initial weights to those from the ResNet18 model\n",
    "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
    "model = torchvision.models.resnet18(weights=weights)\n",
    "\n",
    "# Replace the Fully Connected layer with the dogs classes\n",
    "num_classes = len(train_ds.class_to_idx)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device) # Move model to device CPU/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a6c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m3e-4\u001b[39m) \u001b[38;5;66;03m# 3e-4 is a \"Kaparthy Constant\" (very safe learning rate)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Learning Rate Scheduler\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m scheduler = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4) # 3e-4 is a \"Kaparthy Constant\" (very safe learning rate)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b80a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-----------------------\n",
      "Loss: 2.548060  [16418/16418]\n",
      "Accuracy: 37.8%\n",
      "\n",
      "Epoch 2\n",
      "-----------------------\n",
      "Loss: 4.736163  [16418/16418]\n",
      "Accuracy: 45.1%\n",
      "\n",
      "Epoch 3\n",
      "-----------------------\n",
      "Loss: 3.744511  [16418/16418]\n",
      "Accuracy: 46.0%\n",
      "\n",
      "Epoch 4\n",
      "-----------------------\n",
      "Loss: 3.752618  [16418/16418]\n",
      "Accuracy: 49.6%\n",
      "\n",
      "Epoch 5\n",
      "-----------------------\n",
      "Loss: 3.985560  [16418/16418]\n",
      "Accuracy: 50.1%\n",
      "\n",
      "Epoch 6\n",
      "-----------------------\n",
      "Loss: 3.370225  [16418/16418]\n",
      "Accuracy: 49.5%\n",
      "\n",
      "Epoch 7\n",
      "-----------------------\n",
      "Loss: 5.407432  [16418/16418]\n",
      "Accuracy: 50.4%\n",
      "\n",
      "Epoch 8\n",
      "-----------------------\n",
      "Loss: 3.680038  [16418/16418]\n",
      "Accuracy: 50.9%\n",
      "\n",
      "Epoch 9\n",
      "-----------------------\n",
      "Loss: 3.900916  [16418/16418]\n",
      "Accuracy: 53.5%\n",
      "\n",
      "Epoch 10\n",
      "-----------------------\n",
      "Loss: 3.302883  [16418/16418]\n",
      "Accuracy: 54.2%\n",
      "\n",
      "Epoch 11\n",
      "-----------------------\n",
      "Loss: 5.238008  [16418/16418]\n",
      "Accuracy: 53.2%\n",
      "\n",
      "Epoch 12\n",
      "-----------------------\n",
      "Loss: 3.101556  [16418/16418]\n",
      "Accuracy: 52.5%\n",
      "\n",
      "Epoch 13\n",
      "-----------------------\n",
      "Loss: 4.078279  [16418/16418]\n",
      "Accuracy: 52.1%\n",
      "\n",
      "Epoch 14\n",
      "-----------------------\n",
      "Loss: 3.263097  [16418/16418]\n",
      "Accuracy: 52.5%\n",
      "\n",
      "Epoch 15\n",
      "-----------------------\n",
      "Loss: 3.406848  [16418/16418]\n",
      "Accuracy: 56.5%\n",
      "\n",
      "Epoch 16\n",
      "-----------------------\n",
      "Loss: 2.928273  [16418/16418]\n",
      "Accuracy: 54.9%\n",
      "\n",
      "Epoch 17\n",
      "-----------------------\n",
      "Loss: 2.782479  [16418/16418]\n",
      "Accuracy: 53.8%\n",
      "\n",
      "Epoch 18\n",
      "-----------------------\n",
      "Loss: 4.363035  [16418/16418]\n",
      "Accuracy: 55.0%\n",
      "\n",
      "Epoch 19\n",
      "-----------------------\n",
      "Loss: 3.850153  [16418/16418]\n",
      "Accuracy: 55.0%\n",
      "\n",
      "Epoch 20\n",
      "-----------------------\n",
      "Loss: 3.127279  [16418/16418]\n",
      "Accuracy: 53.0%\n",
      "\n",
      "Epoch 21\n",
      "-----------------------\n",
      "Loss: 2.585804  [16418/16418]\n",
      "Accuracy: 55.5%\n",
      "\n",
      "Epoch 22\n",
      "-----------------------\n",
      "Loss: 4.549522  [16418/16418]\n",
      "Accuracy: 54.6%\n",
      "\n",
      "Epoch 23\n",
      "-----------------------\n",
      "Loss: 1.084357  [16418/16418]\n",
      "Accuracy: 55.4%\n",
      "\n",
      "Epoch 24\n",
      "-----------------------\n",
      "Loss: 2.057647  [16418/16418]\n",
      "Accuracy: 56.2%\n",
      "\n",
      "Epoch 25\n",
      "-----------------------\n",
      "Loss: 3.602483  [16418/16418]\n",
      "Accuracy: 55.9%\n",
      "\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Sets model to training mode, enabling Dropout/Batchnorm\n",
    "\n",
    "    current = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X is the input, y is the answer\n",
    "        X, y = X.to(device), y.to(device) # Move to device\n",
    "\n",
    "        pred = model(X) # Forward pass essentially just run the inputs through the nn\n",
    "        loss = loss_fn(pred, y) # Figure out loss based on the prediction (pred) and the answer (y)\n",
    "        loss.backward() # Backpropogation: go back and figure out which weights effect the loss the most\n",
    "        optimizer.step() # Tweak the weights down the gradient or something COME BACK TO THIS\n",
    "        optimizer.zero_grad() # Resets gradients\n",
    "\n",
    "        current += len(X)\n",
    "        loss_value = loss.item()\n",
    "        print(f\"Loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\", end='\\r', flush=True)\n",
    "    print()\n",
    "\n",
    "# Testing Loop\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval() # Sets model to evaluation mode, disabling Dropout/Batchnorm\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # No gradients are required, saves memory\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device) # Move to device\n",
    "            pred = model(X)\n",
    "\n",
    "            # Sums up the correct predictions, pred.argmax(1) gets the index of the highest probability\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    print(f\"Accuracy: {correct / len(dataloader.dataset):.01%}\")\n",
    "    return correct / len(dataloader.dataset)\n",
    "\n",
    "# Train\n",
    "acc = 0\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-----------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    acc = test_loop(val_loader, model, loss_fn)\n",
    "    print()\n",
    "    scheduler.step(acc)\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4abb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"class_to_idx\": train_ds.class_to_idx\n",
    "}, f\"resnet18_dogs_cpu_{(acc*100):.1f}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
